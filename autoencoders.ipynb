{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train autoencoder for classification with no compression in the bottleneck layer\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from matplotlib import pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "Epoch 1/200\n",
      "42/42 - 2s - loss: 0.2270 - val_loss: 0.1722\n",
      "Epoch 2/200\n",
      "42/42 - 0s - loss: 0.0397 - val_loss: 0.1032\n",
      "Epoch 3/200\n",
      "42/42 - 0s - loss: 0.0255 - val_loss: 0.0596\n",
      "Epoch 4/200\n",
      "42/42 - 0s - loss: 0.0192 - val_loss: 0.0327\n",
      "Epoch 5/200\n",
      "42/42 - 0s - loss: 0.0169 - val_loss: 0.0207\n",
      "Epoch 6/200\n",
      "42/42 - 0s - loss: 0.0145 - val_loss: 0.0147\n",
      "Epoch 7/200\n",
      "42/42 - 0s - loss: 0.0127 - val_loss: 0.0106\n",
      "Epoch 8/200\n",
      "42/42 - 0s - loss: 0.0117 - val_loss: 0.0090\n",
      "Epoch 9/200\n",
      "42/42 - 0s - loss: 0.0116 - val_loss: 0.0087\n",
      "Epoch 10/200\n",
      "42/42 - 0s - loss: 0.0107 - val_loss: 0.0073\n",
      "Epoch 11/200\n",
      "42/42 - 0s - loss: 0.0097 - val_loss: 0.0063\n",
      "Epoch 12/200\n",
      "42/42 - 0s - loss: 0.0099 - val_loss: 0.0062\n",
      "Epoch 13/200\n",
      "42/42 - 0s - loss: 0.0091 - val_loss: 0.0053\n",
      "Epoch 14/200\n",
      "42/42 - 0s - loss: 0.0090 - val_loss: 0.0054\n",
      "Epoch 15/200\n",
      "42/42 - 0s - loss: 0.0083 - val_loss: 0.0052\n",
      "Epoch 16/200\n",
      "42/42 - 0s - loss: 0.0081 - val_loss: 0.0056\n",
      "Epoch 17/200\n",
      "42/42 - 0s - loss: 0.0081 - val_loss: 0.0045\n",
      "Epoch 18/200\n",
      "42/42 - 0s - loss: 0.0083 - val_loss: 0.0052\n",
      "Epoch 19/200\n",
      "42/42 - 0s - loss: 0.0080 - val_loss: 0.0046\n",
      "Epoch 20/200\n",
      "42/42 - 0s - loss: 0.0075 - val_loss: 0.0048\n",
      "Epoch 21/200\n",
      "42/42 - 0s - loss: 0.0075 - val_loss: 0.0045\n",
      "Epoch 22/200\n",
      "42/42 - 0s - loss: 0.0078 - val_loss: 0.0065\n",
      "Epoch 23/200\n",
      "42/42 - 0s - loss: 0.0071 - val_loss: 0.0044\n",
      "Epoch 24/200\n",
      "42/42 - 0s - loss: 0.0071 - val_loss: 0.0041\n",
      "Epoch 25/200\n",
      "42/42 - 0s - loss: 0.0072 - val_loss: 0.0044\n",
      "Epoch 26/200\n",
      "42/42 - 0s - loss: 0.0073 - val_loss: 0.0041\n",
      "Epoch 27/200\n",
      "42/42 - 0s - loss: 0.0067 - val_loss: 0.0036\n",
      "Epoch 28/200\n",
      "42/42 - 0s - loss: 0.0064 - val_loss: 0.0034\n",
      "Epoch 29/200\n",
      "42/42 - 0s - loss: 0.0067 - val_loss: 0.0035\n",
      "Epoch 30/200\n",
      "42/42 - 0s - loss: 0.0068 - val_loss: 0.0035\n",
      "Epoch 31/200\n",
      "42/42 - 0s - loss: 0.0067 - val_loss: 0.0041\n",
      "Epoch 32/200\n",
      "42/42 - 0s - loss: 0.0064 - val_loss: 0.0033\n",
      "Epoch 33/200\n",
      "42/42 - 0s - loss: 0.0061 - val_loss: 0.0030\n",
      "Epoch 34/200\n",
      "42/42 - 0s - loss: 0.0059 - val_loss: 0.0032\n",
      "Epoch 35/200\n",
      "42/42 - 0s - loss: 0.0057 - val_loss: 0.0029\n",
      "Epoch 36/200\n",
      "42/42 - 0s - loss: 0.0062 - val_loss: 0.0031\n",
      "Epoch 37/200\n",
      "42/42 - 0s - loss: 0.0058 - val_loss: 0.0027\n",
      "Epoch 38/200\n",
      "42/42 - 0s - loss: 0.0060 - val_loss: 0.0034\n",
      "Epoch 39/200\n",
      "42/42 - 0s - loss: 0.0059 - val_loss: 0.0035\n",
      "Epoch 40/200\n",
      "42/42 - 0s - loss: 0.0056 - val_loss: 0.0029\n",
      "Epoch 41/200\n",
      "42/42 - 0s - loss: 0.0056 - val_loss: 0.0023\n",
      "Epoch 42/200\n",
      "42/42 - 0s - loss: 0.0056 - val_loss: 0.0031\n",
      "Epoch 43/200\n",
      "42/42 - 0s - loss: 0.0054 - val_loss: 0.0032\n",
      "Epoch 44/200\n",
      "42/42 - 0s - loss: 0.0062 - val_loss: 0.0037\n",
      "Epoch 45/200\n",
      "42/42 - 0s - loss: 0.0053 - val_loss: 0.0027\n",
      "Epoch 46/200\n",
      "42/42 - 0s - loss: 0.0047 - val_loss: 0.0019\n",
      "Epoch 47/200\n",
      "42/42 - 0s - loss: 0.0049 - val_loss: 0.0025\n",
      "Epoch 48/200\n",
      "42/42 - 0s - loss: 0.0051 - val_loss: 0.0029\n",
      "Epoch 49/200\n",
      "42/42 - 0s - loss: 0.0050 - val_loss: 0.0027\n",
      "Epoch 50/200\n",
      "42/42 - 0s - loss: 0.0055 - val_loss: 0.0031\n",
      "Epoch 51/200\n",
      "42/42 - 0s - loss: 0.0055 - val_loss: 0.0027\n",
      "Epoch 52/200\n",
      "42/42 - 0s - loss: 0.0050 - val_loss: 0.0028\n",
      "Epoch 53/200\n",
      "42/42 - 0s - loss: 0.0049 - val_loss: 0.0023\n",
      "Epoch 54/200\n",
      "42/42 - 0s - loss: 0.0054 - val_loss: 0.0021\n",
      "Epoch 55/200\n",
      "42/42 - 0s - loss: 0.0050 - val_loss: 0.0024\n",
      "Epoch 56/200\n",
      "42/42 - 0s - loss: 0.0050 - val_loss: 0.0016\n",
      "Epoch 57/200\n",
      "42/42 - 0s - loss: 0.0053 - val_loss: 0.0021\n",
      "Epoch 58/200\n",
      "42/42 - 0s - loss: 0.0054 - val_loss: 0.0028\n",
      "Epoch 59/200\n",
      "42/42 - 0s - loss: 0.0052 - val_loss: 0.0030\n",
      "Epoch 60/200\n",
      "42/42 - 0s - loss: 0.0049 - val_loss: 0.0038\n",
      "Epoch 61/200\n",
      "42/42 - 0s - loss: 0.0051 - val_loss: 0.0021\n",
      "Epoch 62/200\n",
      "42/42 - 0s - loss: 0.0049 - val_loss: 0.0025\n",
      "Epoch 63/200\n",
      "42/42 - 0s - loss: 0.0047 - val_loss: 0.0033\n",
      "Epoch 64/200\n",
      "42/42 - 0s - loss: 0.0047 - val_loss: 0.0024\n",
      "Epoch 65/200\n",
      "42/42 - 0s - loss: 0.0046 - val_loss: 0.0021\n",
      "Epoch 66/200\n",
      "42/42 - 0s - loss: 0.0048 - val_loss: 0.0031\n",
      "Epoch 67/200\n",
      "42/42 - 0s - loss: 0.0047 - val_loss: 0.0029\n",
      "Epoch 68/200\n",
      "42/42 - 0s - loss: 0.0046 - val_loss: 0.0025\n",
      "Epoch 69/200\n",
      "42/42 - 0s - loss: 0.0049 - val_loss: 0.0028\n",
      "Epoch 70/200\n",
      "42/42 - 0s - loss: 0.0046 - val_loss: 0.0022\n",
      "Epoch 71/200\n",
      "42/42 - 0s - loss: 0.0043 - val_loss: 0.0021\n",
      "Epoch 72/200\n",
      "42/42 - 0s - loss: 0.0046 - val_loss: 0.0020\n",
      "Epoch 73/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0021\n",
      "Epoch 74/200\n",
      "42/42 - 0s - loss: 0.0044 - val_loss: 0.0016\n",
      "Epoch 75/200\n",
      "42/42 - 0s - loss: 0.0045 - val_loss: 0.0017\n",
      "Epoch 76/200\n",
      "42/42 - 0s - loss: 0.0046 - val_loss: 0.0021\n",
      "Epoch 77/200\n",
      "42/42 - 0s - loss: 0.0041 - val_loss: 0.0016\n",
      "Epoch 78/200\n",
      "42/42 - 0s - loss: 0.0045 - val_loss: 0.0021\n",
      "Epoch 79/200\n",
      "42/42 - 0s - loss: 0.0041 - val_loss: 0.0017\n",
      "Epoch 80/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0023\n",
      "Epoch 81/200\n",
      "42/42 - 0s - loss: 0.0041 - val_loss: 0.0016\n",
      "Epoch 82/200\n",
      "42/42 - 0s - loss: 0.0044 - val_loss: 0.0022\n",
      "Epoch 83/200\n",
      "42/42 - 0s - loss: 0.0043 - val_loss: 0.0017\n",
      "Epoch 84/200\n",
      "42/42 - 0s - loss: 0.0042 - val_loss: 0.0027\n",
      "Epoch 85/200\n",
      "42/42 - 0s - loss: 0.0042 - val_loss: 0.0016\n",
      "Epoch 86/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0022\n",
      "Epoch 87/200\n",
      "42/42 - 0s - loss: 0.0045 - val_loss: 0.0020\n",
      "Epoch 88/200\n",
      "42/42 - 0s - loss: 0.0041 - val_loss: 0.0021\n",
      "Epoch 89/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0020\n",
      "Epoch 90/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0023\n",
      "Epoch 91/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0016\n",
      "Epoch 92/200\n",
      "42/42 - 0s - loss: 0.0043 - val_loss: 0.0022\n",
      "Epoch 93/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0019\n",
      "Epoch 94/200\n",
      "42/42 - 0s - loss: 0.0041 - val_loss: 0.0015\n",
      "Epoch 95/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0024\n",
      "Epoch 96/200\n",
      "42/42 - 0s - loss: 0.0043 - val_loss: 0.0020\n",
      "Epoch 97/200\n",
      "42/42 - 0s - loss: 0.0041 - val_loss: 0.0012\n",
      "Epoch 98/200\n",
      "42/42 - 0s - loss: 0.0042 - val_loss: 0.0014\n",
      "Epoch 99/200\n",
      "42/42 - 0s - loss: 0.0034 - val_loss: 0.0017\n",
      "Epoch 100/200\n",
      "42/42 - 0s - loss: 0.0039 - val_loss: 0.0027\n",
      "Epoch 101/200\n",
      "42/42 - 0s - loss: 0.0044 - val_loss: 0.0019\n",
      "Epoch 102/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0015\n",
      "Epoch 103/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0015\n",
      "Epoch 104/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0017\n",
      "Epoch 105/200\n",
      "42/42 - 0s - loss: 0.0039 - val_loss: 0.0033\n",
      "Epoch 106/200\n",
      "42/42 - 0s - loss: 0.0039 - val_loss: 0.0020\n",
      "Epoch 107/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0025\n",
      "Epoch 108/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0016\n",
      "Epoch 109/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0021\n",
      "Epoch 110/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0028\n",
      "Epoch 111/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0016\n",
      "Epoch 112/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0020\n",
      "Epoch 113/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0019\n",
      "Epoch 114/200\n",
      "42/42 - 0s - loss: 0.0039 - val_loss: 0.0014\n",
      "Epoch 115/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0016\n",
      "Epoch 116/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 117/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0014\n",
      "Epoch 118/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0015\n",
      "Epoch 119/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 120/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0022\n",
      "Epoch 121/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0017\n",
      "Epoch 122/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0029\n",
      "Epoch 123/200\n",
      "42/42 - 0s - loss: 0.0039 - val_loss: 0.0014\n",
      "Epoch 124/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0018\n",
      "Epoch 125/200\n",
      "42/42 - 0s - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 126/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0018\n",
      "Epoch 127/200\n",
      "42/42 - 0s - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 128/200\n",
      "42/42 - 0s - loss: 0.0034 - val_loss: 0.0013\n",
      "Epoch 129/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0014\n",
      "Epoch 130/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0021\n",
      "Epoch 131/200\n",
      "42/42 - 0s - loss: 0.0034 - val_loss: 0.0015\n",
      "Epoch 132/200\n",
      "42/42 - 0s - loss: 0.0039 - val_loss: 0.0015\n",
      "Epoch 133/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0019\n",
      "Epoch 134/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0018\n",
      "Epoch 135/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0014\n",
      "Epoch 136/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0012\n",
      "Epoch 137/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0015\n",
      "Epoch 138/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0013\n",
      "Epoch 139/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0019\n",
      "Epoch 140/200\n",
      "42/42 - 0s - loss: 0.0034 - val_loss: 0.0024\n",
      "Epoch 141/200\n",
      "42/42 - 0s - loss: 0.0034 - val_loss: 0.0018\n",
      "Epoch 142/200\n",
      "42/42 - 0s - loss: 0.0035 - val_loss: 0.0012\n",
      "Epoch 143/200\n",
      "42/42 - 0s - loss: 0.0034 - val_loss: 0.0016\n",
      "Epoch 144/200\n",
      "42/42 - 0s - loss: 0.0034 - val_loss: 0.0014\n",
      "Epoch 145/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0016\n",
      "Epoch 146/200\n",
      "42/42 - 0s - loss: 0.0032 - val_loss: 0.0014\n",
      "Epoch 147/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0020\n",
      "Epoch 148/200\n",
      "42/42 - 0s - loss: 0.0035 - val_loss: 0.0011\n",
      "Epoch 149/200\n",
      "42/42 - 0s - loss: 0.0031 - val_loss: 0.0016\n",
      "Epoch 150/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0015\n",
      "Epoch 151/200\n",
      "42/42 - 0s - loss: 0.0034 - val_loss: 0.0020\n",
      "Epoch 152/200\n",
      "42/42 - 0s - loss: 0.0032 - val_loss: 0.0017\n",
      "Epoch 153/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0014\n",
      "Epoch 154/200\n",
      "42/42 - 0s - loss: 0.0032 - val_loss: 0.0015\n",
      "Epoch 155/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0015\n",
      "Epoch 156/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0014\n",
      "Epoch 157/200\n",
      "42/42 - 0s - loss: 0.0034 - val_loss: 0.0010\n",
      "Epoch 158/200\n",
      "42/42 - 0s - loss: 0.0032 - val_loss: 0.0011\n",
      "Epoch 159/200\n",
      "42/42 - 0s - loss: 0.0034 - val_loss: 0.0016\n",
      "Epoch 160/200\n",
      "42/42 - 0s - loss: 0.0031 - val_loss: 0.0014\n",
      "Epoch 161/200\n",
      "42/42 - 0s - loss: 0.0031 - val_loss: 0.0011\n",
      "Epoch 162/200\n",
      "42/42 - 0s - loss: 0.0032 - val_loss: 0.0015\n",
      "Epoch 163/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0016\n",
      "Epoch 164/200\n",
      "42/42 - 0s - loss: 0.0031 - val_loss: 0.0018\n",
      "Epoch 165/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0015\n",
      "Epoch 166/200\n",
      "42/42 - 0s - loss: 0.0032 - val_loss: 0.0015\n",
      "Epoch 167/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 0.0011\n",
      "Epoch 168/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 0.0014\n",
      "Epoch 169/200\n",
      "42/42 - 0s - loss: 0.0035 - val_loss: 0.0013\n",
      "Epoch 170/200\n",
      "42/42 - 0s - loss: 0.0032 - val_loss: 0.0015\n",
      "Epoch 171/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0014\n",
      "Epoch 172/200\n",
      "42/42 - 0s - loss: 0.0032 - val_loss: 0.0016\n",
      "Epoch 173/200\n",
      "42/42 - 0s - loss: 0.0031 - val_loss: 0.0016\n",
      "Epoch 174/200\n",
      "42/42 - 0s - loss: 0.0028 - val_loss: 0.0013\n",
      "Epoch 175/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 0.0011\n",
      "Epoch 176/200\n",
      "42/42 - 0s - loss: 0.0032 - val_loss: 8.6055e-04\n",
      "Epoch 177/200\n",
      "42/42 - 0s - loss: 0.0029 - val_loss: 0.0014\n",
      "Epoch 178/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 0.0013\n",
      "Epoch 179/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0015\n",
      "Epoch 180/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 0.0011\n",
      "Epoch 181/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 8.3176e-04\n",
      "Epoch 182/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 0.0013\n",
      "Epoch 183/200\n",
      "42/42 - 0s - loss: 0.0028 - val_loss: 0.0012\n",
      "Epoch 184/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0012\n",
      "Epoch 185/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0011\n",
      "Epoch 186/200\n",
      "42/42 - 0s - loss: 0.0028 - val_loss: 0.0011\n",
      "Epoch 187/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 0.0012\n",
      "Epoch 188/200\n",
      "42/42 - 0s - loss: 0.0028 - val_loss: 0.0014\n",
      "Epoch 189/200\n",
      "42/42 - 0s - loss: 0.0032 - val_loss: 0.0018\n",
      "Epoch 190/200\n",
      "42/42 - 0s - loss: 0.0027 - val_loss: 0.0011\n",
      "Epoch 191/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 0.0014\n",
      "Epoch 192/200\n",
      "42/42 - 0s - loss: 0.0028 - val_loss: 0.0012\n",
      "Epoch 193/200\n",
      "42/42 - 0s - loss: 0.0031 - val_loss: 0.0014\n",
      "Epoch 194/200\n",
      "42/42 - 0s - loss: 0.0028 - val_loss: 0.0012\n",
      "Epoch 195/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 0.0011\n",
      "Epoch 196/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 0.0012\n",
      "Epoch 197/200\n",
      "42/42 - 0s - loss: 0.0031 - val_loss: 9.8049e-04\n",
      "Epoch 198/200\n",
      "42/42 - 0s - loss: 0.0029 - val_loss: 0.0018\n",
      "Epoch 199/200\n",
      "42/42 - 0s - loss: 0.0031 - val_loss: 0.0010\n",
      "Epoch 200/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 0.0011\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\n",
    "# number of input columns\n",
    "n_inputs = X.shape[1]\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# scale data\n",
    "t = MinMaxScaler()\n",
    "t.fit(X_train)\n",
    "X_train = t.transform(X_train)\n",
    "X_test = t.transform(X_test)\n",
    "# define encoder\n",
    "visible = Input(shape=(n_inputs,))\n",
    "# encoder level 1\n",
    "e = Dense(n_inputs*2)(visible)\n",
    "e = BatchNormalization()(e)\n",
    "e = LeakyReLU()(e)\n",
    "# encoder level 2\n",
    "e = Dense(n_inputs)(e)\n",
    "e = BatchNormalization()(e)\n",
    "e = LeakyReLU()(e)\n",
    "# bottleneck\n",
    "n_bottleneck = n_inputs\n",
    "bottleneck = Dense(n_bottleneck)(e)\n",
    "# define decoder, level 1\n",
    "d = Dense(n_inputs)(bottleneck)\n",
    "d = BatchNormalization()(d)\n",
    "d = LeakyReLU()(d)\n",
    "# decoder level 2\n",
    "d = Dense(n_inputs*2)(d)\n",
    "d = BatchNormalization()(d)\n",
    "d = LeakyReLU()(d)\n",
    "# output layer\n",
    "output = Dense(n_inputs, activation='linear')(d)\n",
    "# define autoencoder model\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# compile autoencoder model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# plot the autoencoder\n",
    "plot_model(model, 'autoencoder_no_compress.png', show_shapes=True)\n",
    "# fit the autoencoder model to reconstruct input\n",
    "history = model.fit(X_train, X_train, epochs=200, batch_size=16, verbose=2, validation_data=(X_test,X_test))\n",
    "# plot loss\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "# define an encoder model (without the decoder)\n",
    "encoder = Model(inputs=visible, outputs=bottleneck)\n",
    "plot_model(encoder, 'encoder_no_compress.png', show_shapes=True)\n",
    "# save the encoder to file\n",
    "encoder.save('encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "Epoch 1/200\n",
      "42/42 - 2s - loss: 0.2269 - val_loss: 0.1841\n",
      "Epoch 2/200\n",
      "42/42 - 0s - loss: 0.0361 - val_loss: 0.0958\n",
      "Epoch 3/200\n",
      "42/42 - 0s - loss: 0.0236 - val_loss: 0.0482\n",
      "Epoch 4/200\n",
      "42/42 - 0s - loss: 0.0191 - val_loss: 0.0281\n",
      "Epoch 5/200\n",
      "42/42 - 0s - loss: 0.0159 - val_loss: 0.0182\n",
      "Epoch 6/200\n",
      "42/42 - 0s - loss: 0.0148 - val_loss: 0.0122\n",
      "Epoch 7/200\n",
      "42/42 - 0s - loss: 0.0135 - val_loss: 0.0104\n",
      "Epoch 8/200\n",
      "42/42 - 0s - loss: 0.0123 - val_loss: 0.0088\n",
      "Epoch 9/200\n",
      "42/42 - 0s - loss: 0.0107 - val_loss: 0.0070\n",
      "Epoch 10/200\n",
      "42/42 - 0s - loss: 0.0115 - val_loss: 0.0073\n",
      "Epoch 11/200\n",
      "42/42 - 0s - loss: 0.0102 - val_loss: 0.0057\n",
      "Epoch 12/200\n",
      "42/42 - 0s - loss: 0.0100 - val_loss: 0.0058\n",
      "Epoch 13/200\n",
      "42/42 - 0s - loss: 0.0093 - val_loss: 0.0048\n",
      "Epoch 14/200\n",
      "42/42 - 0s - loss: 0.0084 - val_loss: 0.0054\n",
      "Epoch 15/200\n",
      "42/42 - 0s - loss: 0.0090 - val_loss: 0.0050\n",
      "Epoch 16/200\n",
      "42/42 - 0s - loss: 0.0083 - val_loss: 0.0047\n",
      "Epoch 17/200\n",
      "42/42 - 0s - loss: 0.0081 - val_loss: 0.0048\n",
      "Epoch 18/200\n",
      "42/42 - 0s - loss: 0.0078 - val_loss: 0.0058\n",
      "Epoch 19/200\n",
      "42/42 - 0s - loss: 0.0076 - val_loss: 0.0046\n",
      "Epoch 20/200\n",
      "42/42 - 0s - loss: 0.0076 - val_loss: 0.0052\n",
      "Epoch 21/200\n",
      "42/42 - 0s - loss: 0.0072 - val_loss: 0.0041\n",
      "Epoch 22/200\n",
      "42/42 - 0s - loss: 0.0076 - val_loss: 0.0036\n",
      "Epoch 23/200\n",
      "42/42 - 0s - loss: 0.0065 - val_loss: 0.0037\n",
      "Epoch 24/200\n",
      "42/42 - 0s - loss: 0.0068 - val_loss: 0.0039\n",
      "Epoch 25/200\n",
      "42/42 - 0s - loss: 0.0068 - val_loss: 0.0043\n",
      "Epoch 26/200\n",
      "42/42 - 0s - loss: 0.0068 - val_loss: 0.0041\n",
      "Epoch 27/200\n",
      "42/42 - 0s - loss: 0.0064 - val_loss: 0.0030\n",
      "Epoch 28/200\n",
      "42/42 - 0s - loss: 0.0069 - val_loss: 0.0036\n",
      "Epoch 29/200\n",
      "42/42 - 0s - loss: 0.0068 - val_loss: 0.0040\n",
      "Epoch 30/200\n",
      "42/42 - 0s - loss: 0.0061 - val_loss: 0.0031\n",
      "Epoch 31/200\n",
      "42/42 - 0s - loss: 0.0065 - val_loss: 0.0035\n",
      "Epoch 32/200\n",
      "42/42 - 0s - loss: 0.0063 - val_loss: 0.0034\n",
      "Epoch 33/200\n",
      "42/42 - 0s - loss: 0.0063 - val_loss: 0.0030\n",
      "Epoch 34/200\n",
      "42/42 - 0s - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 35/200\n",
      "42/42 - 0s - loss: 0.0062 - val_loss: 0.0036\n",
      "Epoch 36/200\n",
      "42/42 - 0s - loss: 0.0059 - val_loss: 0.0039\n",
      "Epoch 37/200\n",
      "42/42 - 0s - loss: 0.0059 - val_loss: 0.0031\n",
      "Epoch 38/200\n",
      "42/42 - 0s - loss: 0.0064 - val_loss: 0.0036\n",
      "Epoch 39/200\n",
      "42/42 - 0s - loss: 0.0061 - val_loss: 0.0030\n",
      "Epoch 40/200\n",
      "42/42 - 0s - loss: 0.0055 - val_loss: 0.0024\n",
      "Epoch 41/200\n",
      "42/42 - 0s - loss: 0.0057 - val_loss: 0.0041\n",
      "Epoch 42/200\n",
      "42/42 - 0s - loss: 0.0057 - val_loss: 0.0030\n",
      "Epoch 43/200\n",
      "42/42 - 0s - loss: 0.0056 - val_loss: 0.0028\n",
      "Epoch 44/200\n",
      "42/42 - 0s - loss: 0.0057 - val_loss: 0.0026\n",
      "Epoch 45/200\n",
      "42/42 - 0s - loss: 0.0059 - val_loss: 0.0031\n",
      "Epoch 46/200\n",
      "42/42 - 0s - loss: 0.0055 - val_loss: 0.0036\n",
      "Epoch 47/200\n",
      "42/42 - 0s - loss: 0.0057 - val_loss: 0.0041\n",
      "Epoch 48/200\n",
      "42/42 - 0s - loss: 0.0055 - val_loss: 0.0029\n",
      "Epoch 49/200\n",
      "42/42 - 0s - loss: 0.0057 - val_loss: 0.0034\n",
      "Epoch 50/200\n",
      "42/42 - 0s - loss: 0.0057 - val_loss: 0.0031\n",
      "Epoch 51/200\n",
      "42/42 - 0s - loss: 0.0049 - val_loss: 0.0024\n",
      "Epoch 52/200\n",
      "42/42 - 0s - loss: 0.0049 - val_loss: 0.0030\n",
      "Epoch 53/200\n",
      "42/42 - 0s - loss: 0.0052 - val_loss: 0.0029\n",
      "Epoch 54/200\n",
      "42/42 - 0s - loss: 0.0052 - val_loss: 0.0019\n",
      "Epoch 55/200\n",
      "42/42 - 0s - loss: 0.0050 - val_loss: 0.0020\n",
      "Epoch 56/200\n",
      "42/42 - 0s - loss: 0.0050 - val_loss: 0.0030\n",
      "Epoch 57/200\n",
      "42/42 - 0s - loss: 0.0051 - val_loss: 0.0030\n",
      "Epoch 58/200\n",
      "42/42 - 0s - loss: 0.0054 - val_loss: 0.0025\n",
      "Epoch 59/200\n",
      "42/42 - 0s - loss: 0.0049 - val_loss: 0.0033\n",
      "Epoch 60/200\n",
      "42/42 - 0s - loss: 0.0050 - val_loss: 0.0029\n",
      "Epoch 61/200\n",
      "42/42 - 0s - loss: 0.0053 - val_loss: 0.0035\n",
      "Epoch 62/200\n",
      "42/42 - 0s - loss: 0.0050 - val_loss: 0.0029\n",
      "Epoch 63/200\n",
      "42/42 - 0s - loss: 0.0051 - val_loss: 0.0021\n",
      "Epoch 64/200\n",
      "42/42 - 0s - loss: 0.0051 - val_loss: 0.0032\n",
      "Epoch 65/200\n",
      "42/42 - 0s - loss: 0.0047 - val_loss: 0.0026\n",
      "Epoch 66/200\n",
      "42/42 - 0s - loss: 0.0051 - val_loss: 0.0026\n",
      "Epoch 67/200\n",
      "42/42 - 0s - loss: 0.0048 - val_loss: 0.0021\n",
      "Epoch 68/200\n",
      "42/42 - 0s - loss: 0.0045 - val_loss: 0.0024\n",
      "Epoch 69/200\n",
      "42/42 - 0s - loss: 0.0047 - val_loss: 0.0023\n",
      "Epoch 70/200\n",
      "42/42 - 0s - loss: 0.0047 - val_loss: 0.0019\n",
      "Epoch 71/200\n",
      "42/42 - 0s - loss: 0.0051 - val_loss: 0.0032\n",
      "Epoch 72/200\n",
      "42/42 - 0s - loss: 0.0053 - val_loss: 0.0029\n",
      "Epoch 73/200\n",
      "42/42 - 0s - loss: 0.0047 - val_loss: 0.0017\n",
      "Epoch 74/200\n",
      "42/42 - 0s - loss: 0.0044 - val_loss: 0.0021\n",
      "Epoch 75/200\n",
      "42/42 - 0s - loss: 0.0044 - val_loss: 0.0018\n",
      "Epoch 76/200\n",
      "42/42 - 0s - loss: 0.0049 - val_loss: 0.0030\n",
      "Epoch 77/200\n",
      "42/42 - 0s - loss: 0.0046 - val_loss: 0.0019\n",
      "Epoch 78/200\n",
      "42/42 - 0s - loss: 0.0048 - val_loss: 0.0030\n",
      "Epoch 79/200\n",
      "42/42 - 0s - loss: 0.0043 - val_loss: 0.0027\n",
      "Epoch 80/200\n",
      "42/42 - 0s - loss: 0.0049 - val_loss: 0.0018\n",
      "Epoch 81/200\n",
      "42/42 - 0s - loss: 0.0043 - val_loss: 0.0018\n",
      "Epoch 82/200\n",
      "42/42 - 0s - loss: 0.0045 - val_loss: 0.0026\n",
      "Epoch 83/200\n",
      "42/42 - 0s - loss: 0.0043 - val_loss: 0.0022\n",
      "Epoch 84/200\n",
      "42/42 - 0s - loss: 0.0044 - val_loss: 0.0019\n",
      "Epoch 85/200\n",
      "42/42 - 0s - loss: 0.0043 - val_loss: 0.0020\n",
      "Epoch 86/200\n",
      "42/42 - 0s - loss: 0.0046 - val_loss: 0.0041\n",
      "Epoch 87/200\n",
      "42/42 - 0s - loss: 0.0045 - val_loss: 0.0018\n",
      "Epoch 88/200\n",
      "42/42 - 0s - loss: 0.0042 - val_loss: 0.0021\n",
      "Epoch 89/200\n",
      "42/42 - 0s - loss: 0.0042 - val_loss: 0.0020\n",
      "Epoch 90/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0023\n",
      "Epoch 91/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0018\n",
      "Epoch 92/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0027\n",
      "Epoch 93/200\n",
      "42/42 - 0s - loss: 0.0044 - val_loss: 0.0023\n",
      "Epoch 94/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0018\n",
      "Epoch 95/200\n",
      "42/42 - 0s - loss: 0.0042 - val_loss: 0.0026\n",
      "Epoch 96/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0017\n",
      "Epoch 97/200\n",
      "42/42 - 0s - loss: 0.0043 - val_loss: 0.0029\n",
      "Epoch 98/200\n",
      "42/42 - 0s - loss: 0.0039 - val_loss: 0.0016\n",
      "Epoch 99/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0013\n",
      "Epoch 100/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0020\n",
      "Epoch 101/200\n",
      "42/42 - 0s - loss: 0.0043 - val_loss: 0.0016\n",
      "Epoch 102/200\n",
      "42/42 - 0s - loss: 0.0044 - val_loss: 0.0017\n",
      "Epoch 103/200\n",
      "42/42 - 0s - loss: 0.0042 - val_loss: 0.0018\n",
      "Epoch 104/200\n",
      "42/42 - 0s - loss: 0.0039 - val_loss: 0.0017\n",
      "Epoch 105/200\n",
      "42/42 - 0s - loss: 0.0039 - val_loss: 0.0016\n",
      "Epoch 106/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0025\n",
      "Epoch 107/200\n",
      "42/42 - 0s - loss: 0.0046 - val_loss: 0.0026\n",
      "Epoch 108/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0019\n",
      "Epoch 109/200\n",
      "42/42 - 0s - loss: 0.0043 - val_loss: 0.0017\n",
      "Epoch 110/200\n",
      "42/42 - 0s - loss: 0.0041 - val_loss: 0.0017\n",
      "Epoch 111/200\n",
      "42/42 - 0s - loss: 0.0040 - val_loss: 0.0026\n",
      "Epoch 112/200\n",
      "42/42 - 0s - loss: 0.0041 - val_loss: 0.0021\n",
      "Epoch 113/200\n",
      "42/42 - 0s - loss: 0.0041 - val_loss: 0.0014\n",
      "Epoch 114/200\n",
      "42/42 - 0s - loss: 0.0041 - val_loss: 0.0025\n",
      "Epoch 115/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0022\n",
      "Epoch 116/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0022\n",
      "Epoch 117/200\n",
      "42/42 - 0s - loss: 0.0042 - val_loss: 0.0019\n",
      "Epoch 118/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0017\n",
      "Epoch 119/200\n",
      "42/42 - 0s - loss: 0.0041 - val_loss: 0.0017\n",
      "Epoch 120/200\n",
      "42/42 - 0s - loss: 0.0041 - val_loss: 0.0019\n",
      "Epoch 121/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0015\n",
      "Epoch 122/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0019\n",
      "Epoch 123/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0021\n",
      "Epoch 124/200\n",
      "42/42 - 0s - loss: 0.0039 - val_loss: 0.0018\n",
      "Epoch 125/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0020\n",
      "Epoch 126/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0015\n",
      "Epoch 127/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0021\n",
      "Epoch 128/200\n",
      "42/42 - 0s - loss: 0.0039 - val_loss: 0.0016\n",
      "Epoch 129/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 130/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0017\n",
      "Epoch 131/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0015\n",
      "Epoch 132/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0019\n",
      "Epoch 133/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0016\n",
      "Epoch 134/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0019\n",
      "Epoch 135/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0017\n",
      "Epoch 136/200\n",
      "42/42 - 0s - loss: 0.0039 - val_loss: 0.0025\n",
      "Epoch 137/200\n",
      "42/42 - 0s - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 138/200\n",
      "42/42 - 0s - loss: 0.0041 - val_loss: 0.0021\n",
      "Epoch 139/200\n",
      "42/42 - 0s - loss: 0.0035 - val_loss: 0.0021\n",
      "Epoch 140/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0017\n",
      "Epoch 141/200\n",
      "42/42 - 0s - loss: 0.0035 - val_loss: 0.0017\n",
      "Epoch 142/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0024\n",
      "Epoch 143/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0019\n",
      "Epoch 144/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0014\n",
      "Epoch 145/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0013\n",
      "Epoch 146/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0017\n",
      "Epoch 147/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0023\n",
      "Epoch 148/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0021\n",
      "Epoch 149/200\n",
      "42/42 - 0s - loss: 0.0035 - val_loss: 0.0019\n",
      "Epoch 150/200\n",
      "42/42 - 0s - loss: 0.0034 - val_loss: 0.0027\n",
      "Epoch 151/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0030\n",
      "Epoch 152/200\n",
      "42/42 - 0s - loss: 0.0038 - val_loss: 0.0020\n",
      "Epoch 153/200\n",
      "42/42 - 0s - loss: 0.0035 - val_loss: 0.0022\n",
      "Epoch 154/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0019\n",
      "Epoch 155/200\n",
      "42/42 - 0s - loss: 0.0031 - val_loss: 0.0012\n",
      "Epoch 156/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0012\n",
      "Epoch 157/200\n",
      "42/42 - 0s - loss: 0.0034 - val_loss: 0.0016\n",
      "Epoch 158/200\n",
      "42/42 - 0s - loss: 0.0034 - val_loss: 0.0018\n",
      "Epoch 159/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0014\n",
      "Epoch 160/200\n",
      "42/42 - 0s - loss: 0.0035 - val_loss: 0.0017\n",
      "Epoch 161/200\n",
      "42/42 - 0s - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 162/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0011\n",
      "Epoch 163/200\n",
      "42/42 - 0s - loss: 0.0032 - val_loss: 0.0021\n",
      "Epoch 164/200\n",
      "42/42 - 0s - loss: 0.0032 - val_loss: 0.0018\n",
      "Epoch 165/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0020\n",
      "Epoch 166/200\n",
      "42/42 - 0s - loss: 0.0037 - val_loss: 0.0016\n",
      "Epoch 167/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0016\n",
      "Epoch 168/200\n",
      "42/42 - 0s - loss: 0.0029 - val_loss: 0.0011\n",
      "Epoch 169/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0011\n",
      "Epoch 170/200\n",
      "42/42 - 0s - loss: 0.0032 - val_loss: 0.0023\n",
      "Epoch 171/200\n",
      "42/42 - 0s - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 172/200\n",
      "42/42 - 0s - loss: 0.0032 - val_loss: 0.0015\n",
      "Epoch 173/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 0.0014\n",
      "Epoch 174/200\n",
      "42/42 - 0s - loss: 0.0034 - val_loss: 0.0016\n",
      "Epoch 175/200\n",
      "42/42 - 0s - loss: 0.0031 - val_loss: 0.0011\n",
      "Epoch 176/200\n",
      "42/42 - 0s - loss: 0.0031 - val_loss: 0.0012\n",
      "Epoch 177/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0015\n",
      "Epoch 178/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 0.0016\n",
      "Epoch 179/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0021\n",
      "Epoch 180/200\n",
      "42/42 - 0s - loss: 0.0034 - val_loss: 0.0016\n",
      "Epoch 181/200\n",
      "42/42 - 0s - loss: 0.0032 - val_loss: 0.0016\n",
      "Epoch 182/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0013\n",
      "Epoch 183/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 0.0019\n",
      "Epoch 184/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 0.0017\n",
      "Epoch 185/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0012\n",
      "Epoch 186/200\n",
      "42/42 - 0s - loss: 0.0032 - val_loss: 0.0012\n",
      "Epoch 187/200\n",
      "42/42 - 0s - loss: 0.0031 - val_loss: 0.0013\n",
      "Epoch 188/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0010\n",
      "Epoch 189/200\n",
      "42/42 - 0s - loss: 0.0031 - val_loss: 0.0013\n",
      "Epoch 190/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0016\n",
      "Epoch 191/200\n",
      "42/42 - 0s - loss: 0.0029 - val_loss: 0.0018\n",
      "Epoch 192/200\n",
      "42/42 - 0s - loss: 0.0033 - val_loss: 0.0014\n",
      "Epoch 193/200\n",
      "42/42 - 0s - loss: 0.0031 - val_loss: 0.0013\n",
      "Epoch 194/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 0.0011\n",
      "Epoch 195/200\n",
      "42/42 - 0s - loss: 0.0031 - val_loss: 0.0014\n",
      "Epoch 196/200\n",
      "42/42 - 0s - loss: 0.0030 - val_loss: 0.0010\n",
      "Epoch 197/200\n",
      "42/42 - 0s - loss: 0.0036 - val_loss: 0.0016\n",
      "Epoch 198/200\n",
      "42/42 - 0s - loss: 0.0031 - val_loss: 0.0016\n",
      "Epoch 199/200\n",
      "42/42 - 0s - loss: 0.0032 - val_loss: 0.0012\n",
      "Epoch 200/200\n",
      "42/42 - 0s - loss: 0.0029 - val_loss: 0.0011\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXhcd33v8ff3zKKRRpK1e5NtycZ2cOLgJI4DJARSCHZYslxCCDQt3NKm3NtAWi7chksbSvrcXpYHHppeIEBxS8vWQKC4F0MWSCDNhp3ESezEjmVHtuVVlqx1pNEsv/vHOZLHsmzLtqRRjj+v59HjmTNn+c6Z8ef8zu+cOcecc4iISHh5xS5AREQml4JeRCTkFPQiIiGnoBcRCTkFvYhIyEWLXcBodXV1rqmpqdhliIi8qjz99NOHnXP1Y7027YK+qamJjRs3FrsMEZFXFTPbdaLX1HUjIhJyCnoRkZBT0IuIhNy066MXETkTmUyGtrY2BgcHi13KpEokEjQ2NhKLxcY9jYJeREKhra2NiooKmpqaMLNilzMpnHN0dHTQ1tZGc3PzuKdT142IhMLg4CC1tbWhDXkAM6O2tva091oU9CISGmEO+WFn8h5DE/T96SxffmAbm/Z0FbsUEZFpJTRBP5jJcfevW3i+TUEvIlOvq6uLr33ta6c93Tve8Q66uiY3t0IT9F6wO5PP60YqIjL1ThT02Wz2pNOtX7+eqqqqySoLCNFZN8NBn1POi0gR3HHHHezYsYMVK1YQi8VIJBJUV1ezdetWXn75Za6//nr27NnD4OAgt99+O7feeitw9LIvfX19XHPNNVxxxRU8/vjjzJ07l5/97GeUlpaedW3hCfpg30S3RhSRz/7HFl7c1zOh81w2p5LPvPv8E77+uc99js2bN7Np0yYeeeQR3vnOd7J58+aR0yDXrl1LTU0NAwMDXHrppbznPe+htrb2mHls376dH/zgB3zrW9/ipptu4r777uOWW24569rDE/TDXTcKehGZBlatWnXMue533303P/3pTwHYs2cP27dvPy7om5ubWbFiBQCXXHIJra2tE1JL6II+ly9yISJSdCdreU+VZDI58viRRx7hoYce4oknnqCsrIy3vOUtY54LX1JSMvI4EokwMDAwIbWE52Bs8E7UoheRYqioqKC3t3fM17q7u6murqasrIytW7fy5JNPTmltoWvRq49eRIqhtraWyy+/nAsuuIDS0lJmzpw58tqaNWu45557eO1rX8vSpUt5/etfP6W1hS7o1XUjIsXy/e9/f8zhJSUl/OIXvxjzteF++Lq6OjZv3jwy/BOf+MSE1RWerpvgV8HquhEROVZogt7MMFPXjYjIaKEJeoCIGTkFvYjIMUIV9J4ZugKCiMixQhX0ZrrWjYjIaKEK+ohnOhgrIjJKqIJeXTciUixnepligK985SukUqkJruioUAW9GeSU9CJSBNM56EPzgynwu250eqWIFEPhZYqvvvpqGhoauPfee0mn09xwww189rOfpb+/n5tuuom2tjZyuRx//dd/zcGDB9m3bx9XXXUVdXV1PPzwwxNeW6iCXl03IgLAL+6AAy9M7DxnLYdrPnfClwsvU/zAAw/w4x//mN/97nc457j22mv57W9/S3t7O3PmzOHnP/854F8DZ8aMGXz5y1/m4Ycfpq6ubmJrDoSq68YzdB69iBTdAw88wAMPPMBFF13ExRdfzNatW9m+fTvLly/nwQcf5C//8i959NFHmTFjxpTUE7oWvbpuRORkLe+p4JzjU5/6FH/6p3963GvPPPMM69ev56/+6q9461vfyp133jnp9YyrRW9ma8xsm5m1mNkdY7z+cTN70cyeN7NfmdmCgtc+aGbbg78PTmTxo3lm5HVRMxEpgsLLFK9evZq1a9fS19cHwN69ezl06BD79u2jrKyMW265hU9+8pM888wzx007GU7ZojezCPBV4GqgDdhgZuuccy8WjPYssNI5lzKz/wZ8AXifmdUAnwFWAg54Opj2yES/EVDXjYgUT+Fliq+55ho+8IEP8IY3vAGA8vJyvvvd79LS0sInP/lJPM8jFovx9a9/HYBbb72VNWvWMGfOnKIdjF0FtDjndgKY2Q+B64CRoHfOFVb2JDB8k8PVwIPOuc5g2geBNcAPzr7043n6wZSIFNHoyxTffvvtxzxftGgRq1evPm66j370o3z0ox+dtLrG03UzF9hT8LwtGHYiHwaGL7w8rmnN7FYz22hmG9vb28dR0tj8PvoznlxEJJQm9KwbM7sFv5vmi6cznXPum865lc65lfX19We8fE8/mBIROc54gn4vMK/geWMw7Bhm9jbg08C1zrn06Uw7UdR1I3JuOxfOujuT9zieoN8ALDazZjOLAzcD6wpHMLOLgG/gh/yhgpfuB95uZtVmVg28PRg2KdR1I3LuSiQSdHR0hDrsnXN0dHSQSCROa7pTHox1zmXN7Db8gI4Aa51zW8zsLmCjc24dfldNOfAj8+/duts5d61zrtPM/hZ/YwFw1/CB2cmgrhuRc1djYyNtbW2czXG+V4NEIkFjY+NpTTOuH0w559YD60cNu7Pg8dtOMu1aYO1pVXWG/EsgKOhFzkWxWIzm5uZilzEthewSCLrWjYjIaOEKeg+16EVERglV0EfUdSMicpxQBb2p60ZE5DihCvqIZ7o5uIjIKKEKes/URy8iMlp4rkefz5N0/eTy8WJXIiIyrYSnRZ86zD8fvJE390/aD29FRF6VwhP0nr9z4rlskQsREZlewhP0kZj/j4JeROQY4Ql6zw96U9CLiBwjPEGvFr2IyJjCE/RehDymoBcRGSU8QQ/kiOLlc8UuQ0RkWglX0FuECGrRi4gUClXQ5y2qrhsRkVFCFfQ5ixJxmWKXISIyrYQq6LMWJeLURy8iUihUQZ+3qProRURGCVXQ59RHLyJynFAFfV5dNyIixwlf0KvrRkTkGKEK+pxFiarrRkTkGKEKer9Fr64bEZFC4Qt6tehFRI4RuqCPqkUvInKMcAW9FyWqg7EiIscIV9Dr9EoRkeOELujVohcROVaogt556qMXERktVEGvFr2IyPHCFfRejJha9CIixwhZ0KtFLyIyWqiC3nkx9dGLiIwyrqA3szVmts3MWszsjjFev9LMnjGzrJndOOq1nJltCv7WTVThY3HBD6acc5O5GBGRV5XoqUYwswjwVeBqoA3YYGbrnHMvFoy2G/gQ8IkxZjHgnFsxAbWeUt6LEiNH3kHEpmKJIiLT3ymDHlgFtDjndgKY2Q+B64CRoHfOtQav5SehxvHzYkTJkss7Ip6SXkQExtd1MxfYU/C8LRg2Xgkz22hmT5rZ9adV3WnKe1Ei5sjn1U8vIjJsPC36s7XAObfXzBYCvzazF5xzOwpHMLNbgVsB5s+ff8YLcua/HZfNQDx25hWLiITIeFr0e4F5Bc8bg2Hj4pzbG/y7E3gEuGiMcb7pnFvpnFtZX18/3lkfz/PDPZcbOvN5iIiEzHiCfgOw2MyazSwO3AyM6+wZM6s2s5LgcR1wOQV9+xMt7/kt+nwuM1mLEBF51Tll0DvnssBtwP3AS8C9zrktZnaXmV0LYGaXmlkb8F7gG2a2JZj8tcBGM3sOeBj43KizdSZWEPRkFPQiIsPG1UfvnFsPrB817M6Cxxvwu3RGT/c4sPwsaxw3FxnuulHQi4gMC9UvY4db9E599CIiI0IV9C44GOuyCnoRkWHhCvrIcNCr60ZEZFiogn749EqddSMiclTIgn64j15BLyIyLGRBH3TdKOhFREaEKuhdRC16EZHRQhX0R1v0OutGRGRYuII+OOsGtehFREaEK+hHWvS6b6yIyLBQBb1FdDBWRGS0UAU9CnoRkeOEK+iDrhvLK+hFRIaFKui9qE6vFBEZLVRB73R6pYjIcUIV9DZyeqXOuhERGRayoC/xH6iPXkRkRMiCPrhhlvroRURGhCroUdeNiMhxQhX03nDQq+tGRGREuILeMzIuoqAXESkQqqA3M7JEsLy6bkREhoUq6COekSGig7EiIgVCFfSeEbToFfQiIsNCFvRGliio60ZEZETogt7vulHQi4gMC1fQe5B16roRESkUqqCP6KwbEZHjhCrozYwMUSyvq1eKiAwLVdAfPetGLXoRkWGhCnr/PPqo+uhFRAqEKuiHz7pRi15E5KhQBb1ZcNaNU9CLiAwLVdBHPP+sG08tehGREaEKes/URy8iMtq4gt7M1pjZNjNrMbM7xnj9SjN7xsyyZnbjqNc+aGbbg78PTlThY9eps25EREY7ZdCbWQT4KnANsAx4v5ktGzXabuBDwPdHTVsDfAa4DFgFfMbMqs++7LFFgoOxnvroRURGjKdFvwpocc7tdM4NAT8EriscwTnX6px7HsiPmnY18KBzrtM5dwR4EFgzAXWPydMvY0VEjjOeoJ8L7Cl43hYMG49xTWtmt5rZRjPb2N7ePs5ZH2/46pVq0YuIHDUtDsY6577pnFvpnFtZX19/xvPxPMjoomYiIscYT9DvBeYVPG8Mho3H2Ux72oa7btSiFxE5ajxBvwFYbGbNZhYHbgbWjXP+9wNvN7Pq4CDs24Nhk2Ik6NVHLyIy4pRB75zLArfhB/RLwL3OuS1mdpeZXQtgZpeaWRvwXuAbZrYlmLYT+Fv8jcUG4K5g2KTwPEgTJ6KrV4qIjIiOZyTn3Hpg/ahhdxY83oDfLTPWtGuBtWdR47h5ZgwSI+qGIJ/3k19E5BwXqiT0zBh0cf9JdrC4xYiITBMhC3oYoMR/khkobjEiItNEqILezEgz3KJX0IuIQMiCHmDQ1KIXESkUuqDPDLfoFfQiIkAIgz5tOhgrIlIohEE/3HWTKm4hIiLTROiCPjMS9GrRi4hACIN+pOtGLXoRESCEQT9kCf+B+uhFRIAQBn1Gp1eKiBwjdEF/tOtGQS8iAiEM+pEWvX4ZKyIChDDo8xYjj6cWvYhIIHRB73nGkFei0ytFRAKhDPqMlajrRkQkEL6gtyDo1XUjIgKEMugJum4U9CIiEMag99SiFxEpFL6gN2NIffQiIiNCGPT4Qa+zbkREgFAGvTFkcXXdiIgEQhn0aXXdiIiMCF/Qe+q6EREpFLqgj5iRJq7r0YuIBEIX9DbSdaMWvYgIhDDoI97wwdgUOFfsckREii50Qe8ZpBm+VHG6uMWIiEwDoQt6M2OQ4OYjOvNGRCR8QT9yMBZ0Lr2ICCEMes+DQXTfWBGRYeELerXoRUSOEcqgH2nR6xRLEZEwBj0MEvOfqEUvIhLGoC8460ZBLyIyvqA3szVmts3MWszsjjFeLzGzfwtef8rMmoLhTWY2YGabgr97Jrb843meMeCGu24U9CIi0VONYGYR4KvA1UAbsMHM1jnnXiwY7cPAEefca8zsZuDzwPuC13Y451ZMcN0n5HfdqEUvIjJsPC36VUCLc26nc24I+CFw3ahxrgO+Ezz+MfBWM7OJK3P8PDMGFPQiIiPGE/RzgT0Fz9uCYWOO45zLAt1AbfBas5k9a2a/MbM3jbUAM7vVzDaa2cb29vbTegOjeV7hL2N11o2IyGQfjN0PzHfOXQR8HPi+mVWOHsk5903n3Ern3Mr6+vqzWqBnRiofBP1Q/1nNS0QkDMYT9HuBeQXPG4NhY45jZlFgBtDhnEs75zoAnHNPAzuAJWdb9Mn4FzWLQawMUp2TuSgRkVeF8QT9BmCxmTWbWRy4GVg3apx1wAeDxzcCv3bOOTOrDw7mYmYLgcXAzokpfWwRM/IOSNZD/9l1A4mIhMEpz7pxzmXN7DbgfiACrHXObTGzu4CNzrl1wLeBfzWzFqATf2MAcCVwl5llgDzwEefcpDazzYxc3inoRUQCpwx6AOfcemD9qGF3FjweBN47xnT3AfedZY2nxTNwLgj67rapXLSIyLQUul/GRryg66a8HvoPFbscEZGiC13Qmxm54RZ9/2HI54tdkohIUYUu6I/punE5GOwqdkkiIkUVuqAf6bpJBufj96n7RkTObaELeq/wrBvQmTcics4LZdDnnYJeRGRY6II+EfMYGMqRK1PQi4hACIN+fk0Z2bxjXzoB5inoReScF7qgX1CbBGDXkTSU1SroReScF7qgb6orA6C1ox+SDf659CIi57DQBf3MigQlUY9dHf2QrNPplSJyzgtd0HuesaC2jNaOlC5sJiJCCIMe/H76XR39UK6uGxGRUAZ9U20ZuzpS5Mtnw1CvbkAiIue0UAb9gtok6WyeropF/oBDLxa3IBGRIgpl0DcFp1i+4jX5Aw4q6EXk3BXKoF9Q659i+XKqHBJVcGhLkSsSESmeUAb9nKpSapJxHm05DDPPV4teRM5poQz6iGdcv2IuD754kMGapXDoJXCu2GWJiBRFKIMe4H2XziOTc2wcmOOfedO1u9gliYgURWiDfumsCl43r4of76nwB+jMGxE5R4U26AH+4PULeKij1n9ycHNxixERKZJQB/31K+YwZ2YDO70F5Hf+ptjliIgURaiDPhrxuOOa87h/6EJc6+OkevQLWRE594Q66AGuWtpA/LVriJDjf//D19i0p6vYJYmITKnQB72Z8eGbbyYbr+SN+ae56Z4n+N5Tu3A63VJEzhGhD3oAIlGii9/GNSUvcOXCCj7908184kfPM5jJFbsyEZFJFy12AVNmxe/jbfkJ35p7N3df9Rm+8kgrL+3vYfHMcp7d3cVtv/ca3ntJI2ZW7EpFRCaUTbcujJUrV7qNGzdOzsw3/CP8/H/A4tX85nVf5GP3bSOfdzTWlPHS/h4W1SeZV1PGuy6cw7sunE0iFpmcOkREJpiZPe2cWznma+dU0ANsXAv/7+PQeCmD77sXEpXEIh7/8kQrT+zo4OWDvbR2pKivKOEPXr+AlkN95PKOGy9pZOmsCmrL45REtQEQkelFQT/ai+vgx38Ecy+Gd34JchmYcxGY4Zzj8R0d3P2r7Tz1Sic1yTgGdPQPAeAZzK8pY1F9ObNmJABYVF/O3OpSXj7Qy6rmGi5bWDu59YuIjKKgH8uLP4MffQhc3n9+6Z/ANV8As5HA39s1wKzKBHkHj+04zMHuQfZ1D7LjUB872vs41JvGOceRVOaYWV/WXMOBnkH60zliESPiGbMqE8yuKuXZ3UfoGcgwszLBVec1MK+6lNaOFFWlMSpLY+TyjopElNJ4hFzeMbeqlHk1ZTgHm/YcobUjxfyaMlY111BXXnLCtzeYyRGLeEQ8HXMQORco6E9kzwY40gp7n4anvg6xMsjnYOUfQeNK2PMUVM6FWcuhYZk/zZafwuP/AMuuhVW3ws6HOVR9MXtizTTVlrH2sVd4YMtBXtNQTnUyTjaXJ5NztB1JsffIABc2VtFQWcKujhSPtRwmm3eURD3S2fxplR7xjGWzKzmSGiI1lMOA6mScXN5xuDdNbzpLWTzCkpkV9KWzOOeoTZZwqHeQ7oEMEc/j6mUzefeFswHY1z1Ie2+aXD5PdTJOTVmcdDZPOpujsz/D9kO9NFQkuKy5hmjE2N2ZYndnirlVpcwojTGUzZPNO+rKS7hgbiXbD/bRO5hlycxymuqSxCL+CV75vKPtyABP7+6krXMAB7xlaT0XzJmBF2yUUkNZdrb301SXJOoZPYMZGioS5PKO1o5+mmqT2oCJjKKgH49nvwf7N8FgD7xwr9/Sj5VBJnX8uDOXw8EXjj73YnD57XD+DRAv829I3nfQ34C0vwxV82DpO2DhmyEzAAdegO49dM+9in4rZXZlCemcoz+dxTOjL51lYChDbLCDzt0v0tGX5mD1JZw3q5wl1RFae437txzgubYu3pp/kkxyJrvLLqCzf4iIZ9SVl1BXHudw3xDdbVu5LP8sPdFaHuIy6isT1JTF6R7I8MstBxga5wZmZmUJHX1DZPNHvy8Rz8jlT/39iUWMmmSc/nSOvnT2mNfM/CtIl8YiLKgtIxoxXj7Yd1xdC+uS9KWzHOpNM6syweKZ5bT3pplRGiMW8ejsHyJZEiGbzbH88HqesBW4ilksqk/SUJGgIhGlIhFjTlWCqrI4W/f3sGVfD/u7B1g2u5LK0hj96Ry15XF6BjLs6khx3uwKzptVSWUiysHeQXZ3DLDnSIo9nSmGcnkW1JTRVJdkZmUC5yDvHK8c7mf9C/tpqEzwxkW1dPSliUY86pLxkQ2ZYSPvPeIZTbVJ6itK6E9neWb3EVJDOZbPnUF7b5rDfWlKYhEaq0uZX1NGeUkU5yCTz5PNObI5fz0lS6JUlsYwYOuBXsriEZbOqqArlSGdzRGPepREI5RE/b28gz2D9KWzlEQj9A5miEU8zptVwTO7uzjQPcCblzRQkYiSzuZJlkQws5FGS8QzyuIRErEI6WyO7Qf7Rr4HEc+YU1VKdVkMM6M/naW9N82R1BB15SXMnpEgGhn/Wd2ZXJ7dnSlmlMaO24Pt7B8im8/TUJEYc1rn3LQ+i24wk5vQEz4U9KfrcAsMdMLcS2Cw278gWvs28CJQfx4seCO88qgf5AvfDI/dDVt+cvx8vCjULITuvZDph3mXwYHN/mOA0mqoWwL7nvU3KtVNMPMCOPCcf7MUV3Cef+1rYKDLr6v5zbD8RujcCY9+yX992fX+cYbKuRAtgVd+Cy0PwZFXjs5j4VWQHfTf08wL6Glazc6hKur2PUIyEaNs3nI47110DuTo7u6mbvuPSKT2Eispo2TRmxja8Rsy2x6ir+51VKZ2k9j/O9LNb+PIeR8gN2clM577BgOd+3ghsZL6hpkkauazJV3L1v09dPUPkkz4gTsv3svb9n+T8tIS0rMv4/7IFWze18/uzhSZXJ7Lytv5Pfck/1l1LSXZXpa1/5Lne0ppKb+ExUuX858thznUM8hFJfvYna3iiCunNhmnfyjL+3vWcl3fvbSVvZa/m/llth0eorN/iN7B7DEbqfNsN3XlJaSqlrD1YB+ZzBCLYp1sHaqjPtLPB5Ib+afeS+lxyWM+0pmVJcyvKSMW8djVkWJf98DIrQ6MPAsinVzZVMoLqRqe3Z+moaKEXN7R0T9EDT044AiVI/OrIEU/CfLBT1oinhH1bGQPLx71xr0xLnSJbeO26L/z+ez72ermn3TcGFmiZBlg7MA8ygFHg3NuVSmd/UMMjPF7lLiX502RzTTn93DEVfDv+cvJESHqGY3VpTRUJkjEIvQMZOgeyNA7mCWTyxOLeMQjRtdAhmzOkXNuZCOydGYFiXgEnKN/KEfLoT7AP2a2sqma0liE59u6ARjI5Gg93E9VWZyFdUkaa0rZ2d7P3q4BFgd720PZPPu6Bqgqi9FYVcbWg7109qdxDioTMcwgnc3TVJtkZmUJmVye/d2DpIZylJdEKU9Ecc7R3psmFvGIRjx6BzPMqkxQWx6n5VAfhlFTHqc2WF5vOktlIsrzbd1s2dfD8rkzWFBbxmAmR1NtkovmV/POYC/7dJ110JvZGuDvgQjwj865z416vQT4F+ASoAN4n3OuNXjtU8CHgRzwMefc/Sdb1rQI+jPR3Qat/+k3T5P1kKyF2sVQUg6ZQXjs7/09hQWXw5LVUFIJT34N+g75G4DcELRv9TcqDefDvFVQMQtqFkF/Ozz7XaicDZVz/OMLR1r95V50CyQb/LOJBgsu7xAtheYrYfHVsOj3YNt6+O0X/fkl6/yNS3+7P655R49VzLwA6hZD62PQf8ifT24o2OiY36V1cAuU1vgbuW2/8Dc+XhTyWYiXw1Df0TpqFsLAEf+vtNqf/+Htfq3REn+j03ipv6569kL5TP/95dL+MjIpf+MEYBFY/l6YMRe2/fLoLSKrm2H2hf5B9W3r/XW86zH//Uf8VqDLDsKBzaSTc0jHKplx4Al/2mQ9bv4bYf8mrGsXucbL8Lr3YL37yFc1s/uyz9AVa2Be+yPMyB0hOut8iCb89ZHPkslk6R/KYTiSz/8z0cMv+fNNVJE//wa8mmaIJ6H9ZdzT/4TlhnCzV+CW3wT7N+G9cC95L85QaT35eAWxhiV4tQs5nEtSkYhR5lLkew/QVTqPQ9ZAtKuF8p4W4pkeBqoW47k8kWyKjhnLsd69VPRsJzJ7OfWb/5FINkUmmmTn8r+gt3Quyc4tuMwAPYm51LsOEpZhyEqYu/NeIukuNs++kfqSLDWpnRxJGz2ljfSXN5HNZJjf8Vvqel+ivfpi2mreSEv5xbzYXUKzd4ArveeYs+9BopkeUsn5dMRmUdu1mar03pGvQaqiiVRkBkOZLLsjjTzmXcrjkZUkS0tYFmljUa6Vynw3XZFqOrw65nGAuOUYLKnFmq8k0for5rbex8HoHDIWpyrfTa7hfFLJeexo7+Mn7fPYm5vBFXOMP+y5h3mZVloaVvNKpJkdvVFaej2WlKdprIjwUGohnWmPhOW4sLwT+g4R723j6pKXiJeU0Fp6Pl3ZOEcitfTEZzL70G+oHDpEl1fFKxUrOZRcQl86x8DgIA6jrjJJPjtEw9BulnhteH0HSA1l6ahewYHoPPakYhxOZSmL5JgTH2RXOsnyGYNcX/0Ku9u72TlUxfb4MrZ3ZnhdYxX3fuQNZxRBZxX0ZhYBXgauBtqADcD7nXMvFozz34ELnXMfMbObgRucc+8zs2XAD4BVwBzgIWCJc+6EP0l91Qb9VHIO9j7jt9YveI/fBwCQ7oWefX7306zlEDtJCy2fg50PQ6rT3xjEy/2AffRLfrDXLoYr/gLmX+bPt/UxP7Trl0Au6+/dmEE2DS/9B7Q+Citugdmvg/3PQXbA3yvZ8Wt/g1U+09+w7N/kb1iu/Qf/uMcLP4L7/5dfU9V8OLLL32Na9Sfwmy/4G6XVf+eH/VPfgKe/428E6s+DS//Y31Dse9bf+HgRaFwF7/6K/z6e/Jo/Ty/qL7NhGXTsgO49/rTJOn/Pp/Uxf+Ox6K3+by0SlXDFx+HXfwu9+4+us9EbsdFqX+Mftymt9jc42x88Or55sOL3oXoBbP25X3Mk7tfhRf11M3DE33Ps3uNvNIenK62B1OFgIebPI1HlNwwiJRCJQqrD3xDOaISuXf6e4g3fgP/4mN9VODytF/HnbZ6/3NwQLLjC/4w23wclFf53Jzd0dIMMULfU/1x2P+Evt5AXhYVv8Zd9pBU6X/Hn94bboOkKf5rH/73W9RAAAAevSURBVK9fJ/ifVarjaE2cqrEZjFPd5K+jXNZfxz1tI2M4DGbMxQa6/O/krOWw75mxZxcr8ze+qc5j95qTDf66GRjj4oeRuL9OwF/Pw9N5USif5XfV5jPHTwfBZ1jtf1fzWb+Rl+499n1bBBcvIzP7EuIf+tkp1scJFnOWQf8G4G+cc6uD558CcM79n4Jx7g/GecLMosABoB64o3DcwvFOtDwF/TlopO9jGvSnZof8/5iRqL/B3Pu0v6fR9CaYMc8Pl3w22HhE/H9d3t+wzZh/NMyGpfv8vZJIzP/PPuzQVj9squYdX4NzRzcQ0YQ/bd8h6D3gb0ziZUfHGz7IceQVf/6l1f54pTUQjUM+7284evf7G8d40n+cbPDDK90DpVX+/PoPQ2KGv7zh+ad7/PURLz/6+XTthrYNfnCVz4Kmy/3pxiuf8xsAbRv8dVe3BOZc7G94e/b59dUu8vcmO3fC9gegptnfUFrQv28Gfe3+Xmc27XdTduzw180l/9Xfw+tu8+c30OXXWlbjv6eWh/yGQ1mNv+zKuf5eeN0Sf949e/1jaV27/fXa/Ga/gdN/GF7+pb+cSMw/NpdJ+eNXzPL3Vhte62/wchl/A9e919+opTr8z6ZiFhx+2V//S9f46/Xwdn9dDPX7e+yXf2z867LA2Qb9jcAa59wfB8//ALjMOXdbwTibg3Haguc7gMuAvwGedM59Nxj+beAXzrkfj1rGrcCtAPPnz79k165dZ/I+RUTOWScL+mlxUTPn3Dedcyudcyvr6+uLXY6ISKiMJ+j3AoX7l43BsDHHCbpuZuAflB3PtCIiMonGE/QbgMVm1mxmceBmYN2ocdYBHwwe3wj82vl9QuuAm82sxMyagcXA7yamdBERGY9TXqbYOZc1s9uA+/FPr1zrnNtiZncBG51z64BvA/9qZi1AJ/7GgGC8e4EXgSzwZyc740ZERCaefjAlIhIC0/5grIiITB4FvYhIyCnoRURCbtr10ZtZO3A2v5iqAw6fcqypp7pOz3StC6Zvbarr9EzXuuDMalvgnBvzh0jTLujPlpltPNEBiWJSXadnutYF07c21XV6pmtdMPG1qetGRCTkFPQiIiEXxqD/ZrELOAHVdXqma10wfWtTXadnutYFE1xb6ProRUTkWGFs0YuISAEFvYhIyIUm6M1sjZltM7MWM7ujiHXMM7OHzexFM9tiZrcHw//GzPaa2abg7x1Fqq/VzF4IatgYDKsxswfNbHvwb/Wp5jPBNS0tWC+bzKzHzP68GOvMzNaa2aHgZjrDw8ZcP+a7O/jOPW9mF09xXV80s63Bsn9qZlXB8CYzGyhYb/dMVl0nqe2En52ZfSpYZ9vMbPUU1/VvBTW1mtmmYPiUrbOTZMTkfc+cc6/6P/yrau4AFgJx4DlgWZFqmQ1cHDyuwL/f7jL8u219Yhqsq1agbtSwLwB3BI/vAD5f5M/yALCgGOsMuBK4GNh8qvUDvAP4Bf5NTV8PPDXFdb0diAaPP19QV1PheEVaZ2N+dsH/heeAEqA5+H8bmaq6Rr3+JeDOqV5nJ8mISfuehaVFvwpocc7tdM4NAT8EritGIc65/c65Z4LHvcBLwNxi1HIargO+Ezz+DnB9EWt5K7DDOVeU+0k6536Lf6ntQidaP9cB/+J8TwJVZjZ7qupyzj3gnAvuIs6T+Df2mXInWGcnch3wQ+dc2jn3CtCC//93SusyMwNuAn4wGcs+mZNkxKR9z8IS9HOBPQXP25gG4WpmTcBFwFPBoNuCXa+1U909UsABD5jZ0+bfqxdgpnNuf/D4ADCzOKUB/r0MCv/zTYd1dqL1M52+d3+E3+ob1mxmz5rZb8zsTUWqaazPbrqsszcBB51z2wuGTfk6G5URk/Y9C0vQTztmVg7cB/y5c64H+DqwCFgB7MffbSyGK5xzFwPXAH9mZlcWvuj8fcWinHNr/h3MrgV+FAyaLutsRDHXz4mY2afxb+zzvWDQfmC+c+4i4OPA982scorLmnaf3Sjv59gGxZSvszEyYsREf8/CEvTT6t60ZhbD/wC/55z7CYBz7qBzLuecywPfYpJ2V0/FObc3+PcQ8NOgjoPDu4LBv4eKURv+xucZ59zBoMZpsc448fop+vfOzD4EvAv4/SAcCLpFOoLHT+P3gy+ZyrpO8tlNh3UWBf4L8G/Dw6Z6nY2VEUzi9ywsQT+e+9pOiaDv79vAS865LxcML+xTuwHYPHraKagtaWYVw4/xD+Zt5th7/n4Q+NlU1xY4ppU1HdZZ4ETrZx3wh8FZEa8Hugt2vSedma0B/idwrXMuVTC83swiweOF+Pdq3jlVdQXLPdFnNx3uI/02YKtzrm14wFSusxNlBJP5PZuKo8xT8Yd/ZPpl/C3xp4tYxxX4u1zPA5uCv3cA/wq8EAxfB8wuQm0L8c94eA7YMryegFrgV8B24CGgpgi1JYEOYEbBsClfZ/gbmv1ABr8v9MMnWj/4Z0F8NfjOvQCsnOK6WvD7boe/Z/cE474n+Hw3Ac8A7y7COjvhZwd8Olhn24BrprKuYPg/Ax8ZNe6UrbOTZMSkfc90CQQRkZALS9eNiIicgIJeRCTkFPQiIiGnoBcRCTkFvYhIyCnoRURCTkEvIhJy/x/X8gaSNif9vwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# train autoencoder for classification with with compression in the bottleneck layer\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\n",
    "# number of input columns\n",
    "n_inputs = X.shape[1]\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# scale data\n",
    "t = MinMaxScaler()\n",
    "t.fit(X_train)\n",
    "X_train = t.transform(X_train)\n",
    "X_test = t.transform(X_test)\n",
    "# define encoder\n",
    "visible = Input(shape=(n_inputs,))\n",
    "# encoder level 1\n",
    "e = Dense(n_inputs*2)(visible)\n",
    "e = BatchNormalization()(e)\n",
    "e = LeakyReLU()(e)\n",
    "# encoder level 2\n",
    "e = Dense(n_inputs)(e)\n",
    "e = BatchNormalization()(e)\n",
    "e = LeakyReLU()(e)\n",
    "# bottleneck\n",
    "n_bottleneck = round(float(n_inputs) / 2.0)\n",
    "bottleneck = Dense(n_bottleneck)(e)\n",
    "# define decoder, level 1\n",
    "d = Dense(n_inputs)(bottleneck)\n",
    "d = BatchNormalization()(d)\n",
    "d = LeakyReLU()(d)\n",
    "# decoder level 2\n",
    "d = Dense(n_inputs*2)(d)\n",
    "d = BatchNormalization()(d)\n",
    "d = LeakyReLU()(d)\n",
    "# output layer\n",
    "output = Dense(n_inputs, activation='linear')(d)\n",
    "# define autoencoder model\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# compile autoencoder model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# plot the autoencoder\n",
    "plot_model(model, 'autoencoder_compress.png', show_shapes=True)\n",
    "# fit the autoencoder model to reconstruct input\n",
    "history = model.fit(X_train, X_train, epochs=200, batch_size=16, verbose=2, validation_data=(X_test,X_test))\n",
    "# plot loss\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "# define an encoder model (without the decoder)\n",
    "encoder = Model(inputs=visible, outputs=bottleneck)\n",
    "plot_model(encoder, 'encoder_compress.png', show_shapes=True)\n",
    "# save the encoder to file\n",
    "encoder.save('encoder.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline in performance with logistic regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8939393939393939\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# scale data\n",
    "t = MinMaxScaler()\n",
    "t.fit(X_train)\n",
    "X_train = t.transform(X_train)\n",
    "X_test = t.transform(X_test)\n",
    "# define model\n",
    "model = LogisticRegression()\n",
    "# fit model on training set\n",
    "model.fit(X_train, y_train)\n",
    "# make prediction on test set\n",
    "yhat = model.predict(X_test)\n",
    "# calculate accuracy\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the train data\n",
    "X_train_encode = encoder.predict(X_train)\n",
    "# encode the test data\n",
    "X_test_encode = encoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/share/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = LogisticRegression()\n",
    "# fit the model on the training set\n",
    "model.fit(X_train_encode, y_train)\n",
    "# make predictions on the test set\n",
    "yhat = model.predict(X_test_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "0.9393939393939394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/share/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# evaluate logistic regression on encoded input\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# scale data\n",
    "t = MinMaxScaler()\n",
    "t.fit(X_train)\n",
    "X_train = t.transform(X_train)\n",
    "X_test = t.transform(X_test)\n",
    "# load the model from file\n",
    "encoder = load_model('encoder.h5')\n",
    "# encode the train data\n",
    "X_train_encode = encoder.predict(X_train)\n",
    "# encode the test data\n",
    "X_test_encode = encoder.predict(X_test)\n",
    "# define the model\n",
    "model = LogisticRegression()\n",
    "# fit the model on the training set\n",
    "model.fit(X_train_encode, y_train)\n",
    "# make predictions on the test set\n",
    "yhat = model.predict(X_test_encode)\n",
    "# calculate classification accuracy\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
